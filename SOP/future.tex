%!TEX root = main.tex
\subsection*{Future Directions}

% My experiences collectively shaped my research interests in improving human-AI interaction and collaboration, 
% incorporating interdisciplinary knowledge and.
% I hope to continue this line of work in my PhD.
% specific, I am curretly interested in the following problems:

\sect{a research problem I am interested in}

I am drawn toward HAI research because of its interdisciplinary nature and human-centered approach. Specifically, I am currently interested in the following directions:
\noindent \textbf{\\1. Machine teaching and modeling human learners.} 
The problem of AI teaching knowledge to human learners is important, difficult and understudied. For one, I think an effective human learner model is lacking, as existing work often assume simplistic learner models like linear separators. In my past and current work, we are working on modeling human representation space from perceptual judgment annotations, an effective yet costly method. Another problem is the human learning process: while there are decades of psychological research on how humans learn, there is limited work on leveraging those findings for AI teachers. If we can more accurately simulate human learning with AI models, AI teachers can provide better teaching. For example, in my current project on designing AI-driven tutorials for radiology training, we are currently investigating whether teaching with contrastive examples can improve learning.
\noindent \textbf{\\2. The gap between laypeople and real-world practitioners in HAI experiments.}
Most HAI empirical studies use crowdworkers due to their lower cost and higher accessibility compared to real-world practitioners; often they list the potential gap between laypeople and practitioners as a limitation at the end of the paper but do not expand further. I find this extremely unsatisfying and I envision two directions toward this issue. The first is to bridge the gap by bringing laypeoples' experiment environment closer to the actual task's. This could be done by improving their task knowledge through short training sessions, one direction I am currently working on, or by simulating other factors like risk, cognitive engagement, or the user interface. Another direction is to make more sense of the gap. For example, can we design a framework that infers meaningful information on how practitioners would behave (reliance towards AI, what type of explanations are useful) from experiment results on laypeople? An extension to this may be to design different forms of AI assistance for different levels of user expertise; this is motivated by AI's varying effects on people with different expertise [\citenum{wang2021},\citenum{tschandl2020}] and is similar in spirit to disaggregated evaluations [\citenum{barocas2021}].











% \noindent \textbf{\\1. How can we design AI assistance that inspires humans' appropriate reliance?}


% In our project, we spent a large amount of time devising a decision support system that differed from existing literature providing model explanations: our goal was to provide faithful and neutral evidence for humans to make independent decisions. Thus we eventually devised a neutral decision support policy that did not reveal AI's predicted label and provided nearest-neighbor explanations from all classes. 
% We showed such a policy provided effective decision support, but it was less effective than a persuasive decision support policy closer to model explanations. 

% This alludes to a larger problem: the tradeoff between human agency and human-AI team performance. 
% Many human-AI teams provide humans with "persuasive" AI assistance that improves humans performance but at the expense of humans' overreliance and blind trust towards AI [\citenum{bansal2021}]. This is undesired and unethical, especially in high-stake domains where humans should have the last say.
% On the other hand, many work [\citenum{bansal2021}], including ours, also show that neutral, non-persuasive assistance that dissuade humans from blindly following AI perform no better than persuasive assistance. 
% Thus, I am interested in solving this dilemma.



% \noindent \textbf{\\2. AI learning human perception to provide better AI assistance}


% % Our model learns to predict human perception, but in general neural networks learning human perception and intuition is a more difficult task than classification. Perception prediction accuracy is low (70-80\%) compared to classification but the exact reason for this is unknown: it could be due to inter-annotation disagreement, randomness in human perception, low-quality annotation, etc. Related work has also primarily focused on visual perception on images and other modalities are less explored. 
% As our work showed, AI learning human perception can provide better decisions support performance; I believe in general, incorporating more human knowledge and intution to AI frameworks has potential to provide more effective AI assistance, but many issues remain unsolved in this nascent direction. 
% For example, our work focuses on visual perception on images as it is the easiest form of perception for humans; perception for other modalities like audio and text are also important but much more difficult to collect and utilize. The limitations and bounds of AI learning human perception is also unexplored, including the amount of data needed, the format of data (we used triplet annotations), how well neural networks can learn, etc.




% % \textbf{(ii) how and when we can achieve human-AI complementary performance} Complementary performance refers to human-AI team performance outperforming human or AI alone. As mentioned, one factor that affects team performance is human agency and reliance on AI. I am also interested in other factors such as human's and AI expertise on the task. I want to explore the limits of human-AI team and when complementary performance is possible.\\






