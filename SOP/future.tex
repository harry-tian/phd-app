%!TEX root = main.tex

% My experiences collectively shaped my research interests in improving human-AI interaction and collaboration, 
% incorporating interdisciplinary knowledge and.
% I hope to continue this line of work in my PhD.
% specific, I am curretly interested in the following problems:


Thus, I am drawn towards HAI research because of the different perspectives and fields of knowledge it requires and its human-centered approach, differing drastically from ML. 
I hope to continue this line of work in my PhD.
Specifically, I am currently interested in the problem of the gap between laypeople and real-world practitioners in HAI experiments.

\subsect{research problem description}
The gap in experiment results on laypeople and practitioners is a large one that is often downplayed: AI assistance affects people differently depending on their task expertise. [\citenum{wang2021}] suggest explanations may be more effective on tasks that people perceive themselves as more knowledable. [\citenum{tschandl2020}] report that in AI-assisted skin-cancer recognition, clinicians with different expertise show differences in accuracy, confidence, reliance on AI, calibration. 
Most HAI empirical work use crowdworkers due to its their lower cost and higher accessibiilty compared to real-world practitioners; 
often they list the potential gap between laypeople and practitioners as a limitation at the end of the paper, but do not expand further. 
I find this extremely unsatisfying and I envision two direction towards this issue:

\subsect{research goal}
\textbf{1) Bridging the gap.}
One straightforward solution is to reduce the gap between laypeple and real-world practitioners in HAI experimental setting. 
There are many approaches to this, one of which I am currently working on is improving laypeoples' task expertise through machine teaching. 
Such teaching frameworks can be deployed as short, effective training phases before an HAI experiment procedure. 
We are currently investigating whether grouping teaching examples in pairs or tuples can improve teaching by inducing contrastive learning.
Another way to bridge the gap is to simulate practitioners' real-world task environment in terms of risk, cognitive engagement, user-interface, etc.


\textbf{2) Making sense of the gap}
Of course, the gap can never be fully reduced, so another direction is to design a framework that instead infers meaningful information from experiments of laypeople. 
For example, ideally such a framework could infer patterns on practitioners from experiment results on laypeople.
An extension to this may be to design different forms of AI assistance fordifferent levels of user expertise, since growing evidence suggests AI has varying effects on people with different expertise [\citenum{wang2021},\citenum{tschandl2020}]. 
This "disaggregrated assistance" is also motivated by disaggregrated evaluations
% [\citenum{}]
.




% \noindent \textbf{\\1. How can we design AI assistance that inspires humans' appropriate reliance?}


% In our project, we spent a large amount of time devising a decision support system that differed from existing literature providing model explanations: our goal was to provide faithful and neutral evidence for humans to make independent decisions. Thus we eventually devised a neutral decision support policy that did not reveal AI's predicted label and provided nearest-neighbor explanations from all classes. 
% We showed such a policy provided effective decision support, but it was less effective than a persuasive decision support policy closer to model explanations. 

% This alludes to a larger problem: the tradeoff between human agency and human-AI team performance. 
% Many human-AI teams provide humans with "persuasive" AI assistance that improves humans performance but at the expense of humans' overreliance and blind trust towards AI [\citenum{bansal2021}]. This is undesired and unethical, especially in high-stake domains where humans should have the last say.
% On the other hand, many work [\citenum{bansal2021}], including ours, also show that neutral, non-persuasive assistance that dissuade humans from blindly following AI perform no better than persuasive assistance. 
% Thus, I am interested in solving this dilemma.



% \noindent \textbf{\\2. AI learning human perception to provide better AI assistance}


% % Our model learns to predict human perception, but in general neural networks learning human perception and intuition is a more difficult task than classification. Perception prediction accuracy is low (70-80\%) compared to classification but the exact reason for this is unknown: it could be due to inter-annotation disagreement, randomness in human perception, low-quality annotation, etc. Related work has also primarily focused on visual perception on images and other modalities are less explored. 
% As our work showed, AI learning human perception can provide better decisions support performance; I believe in general, incorporating more human knowledge and intution to AI frameworks has potential to provide more effective AI assistance, but many issues remain unsolved in this nascent direction. 
% For example, our work focuses on visual perception on images as it is the easiest form of perception for humans; perception for other modalities like audio and text are also important but much more difficult to collect and utilize. The limitations and bounds of AI learning human perception is also unexplored, including the amount of data needed, the format of data (we used triplet annotations), how well neural networks can learn, etc.




% % \textbf{(ii) how and when we can achieve human-AI complementary performance} Complementary performance refers to human-AI team performance outperforming human or AI alone. As mentioned, one factor that affects team performance is human agency and reliance on AI. I am also interested in other factors such as human's and AI expertise on the task. I want to explore the limits of human-AI team and when complementary performance is possible.\\






