%!TEX root = main.tex

\sect{development of research interest}

In this project, we encountered the tradeoff between human agency and performance: showing neutral and informative explanations would retained human agency, but results in lower task performance than showing persuasive explanations that nudged humans to follow the AI. 
I was intrigued by this dilemma and was drawn to the complexities of human-AI interaction. 

\subsect{old perspective}
Coming from a machine learning background, my perspectives towards research were also largely influenced by ML. 
Specifically, I treated the human-AI system as another ML model: I thought that if we could build a more generalizable model, produce better explanations and achieve higher performance on some metric, we would achieve better human-AI collaboration. 
% To generate our ideal assistance, we would tune hyperparameters in our model until we achieved best performance on our synthetic humans.

% \subsect{new perspective}
However, through a \href{https://github.com/ChicagoHAI/human-centered-machine-learning}{Human-Centered Machine Learning course} and a \href{https://datascience.uchicago.edu/events/human-ai-conference/}{Human+AI conference}, both at UChicago, 
I gradually formed a new, more thorough perspective towards HAI.
I first learned of the many "non-ML" factors that could affect human-AI team performance. On the human side, AI asisstance could alter humans' decision-making process and induce bias [\citenum{green2021}]; lack of human cognitive engagement could cause overreliance or underreliance towards AI [\citenum{bucinca2021}]. On the AI side, explanations may not align with humans' decision-making process [\citenum{yacoby2022}]; the presentation of assistance is also crucicial, as showing too little or too much information may be problematic.


More important, I learned of the misalignments between HAI research and real life scenarios, including misalignment in the task and objective [\citenum{{guerdan2022}}], in diserideta of the assistance, in the subjects that use our assistance. 


A more important lesson I learned was a more human-centered perspective: to take a step back and question whether the task at hand is appropriate for a real-life scenario that humans use. 



% I was used to pursuing an empirical objective like a benchmark, but disregarding the big picture elments of HAI like human and stakeholder needs, how (and should) human use AI, etc. These questions are immensly difficult and open-ended but are also what exictes me about this line of research.


%  For example, the explanations and assistance we ML researchers are designing may not be what real humans, including workers, stalkholders, need. 

% This alludes to a larger problem: the tradeoff between human agency and human-AI team performance. 
% Many human-AI teams provide humans with "persuasive" AI assistance that improves humans performance but at the expense of humans' overreliance and blind trust towards AI [\citenum{bansal2021}]. This is undesired and unethical, especially in high-stake domains where humans should have the last say.
% On the other hand, many work [\citenum{bansal2021}], including ours, also show that neutral, non-persuasive assistance that dissuade humans from blindly following AI perform no better than persuasive assistance. 
% Thus, I am interested in solving this dilemma.








% At UChicago, through Dr. Chenhao Tan's course in Human-Center Machine Learning I learned of the complexities of human-AI interaction. In high-stake domains like medical diagnosis, full automation of AI is often not desired and humans have to make the final decision. I am intrigued by the problems caused by this limitation, such as generating explanations that are informative and neutral instead of persuasive and deceiving, complementary performance etc. 



% In our project, we spent a large amount of time devising a decision support system that differed from existing literature providing model explanations: our goal was to provide faithful and neutral evidence for humans to make independent decisions. Thus we eventually devised a neutral decision support policy that did not reveal AI's predicted label and provided nearest-neighbor explanations from all classes. 
% We showed such a policy provided effective decision support, but it was less effective than a persuasive decision support policy closer to model explanations. 



% \subsect{direct impact}
% Also, I am motivated by the direct and visible impact in human-centered AI research. My research project [\citenum{human-compat}] has improved crowdworkers' performance on pneumonia diagnosis in chest X-rays, meaning it has the potential to be implemented in real-life medical settings. Such human-related experiments and improvements are very rewarding to me.
% % \\ 


% \subsect{interdisciplinary}
% Finally I enjoy the interdisciplinary aspects of human-centered AI research. 
% My past research in human-compatible AI decision-support involved modeling human perception, so we used triplet annotations, a method from experimental psychology. 
% My current research aims to leverage our human-compatible AI build radiology teaching framework and we are exploring psychology literature in learning and categorization. 
% More generally, human-centered AI revolves around how humans interact with a decision-making entity and thus involves many many different fields like economics, sociology, ethics, legal, etc. 
% An exciting aspect of human-centered AI resarch is learning and utilizing knowledge from multiple fields. 
% \\



