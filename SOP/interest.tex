
% \subsection*{Developing a Human-Centered Perspective}

From this work, apart from experiencing the complexities of an entire research project lifecycle, I also began to develop my interest in human-AI interaction. In designing our decision-support policy, we encountered the tradeoff between human agency and performance: policies that achieved the best performance came at the expense of humans’ overreliance on AI. I was intrigued by this dilemma as, coming from an ML background, it could not be solved just by training better models and obtaining higher accuracies. Rather, it required accounting for how humans use and interact with AI assistance. To gain a deeper understanding of HAI, I learned from a
\href{https://github.com/ChicagoHAI/human-centered-machine-learning}{Human-Centered Machine Learning course},
a \href{https://datascience.uchicago.edu/events/human-ai-conference/}{Human+AI conference}, and related papers.
I began to notice problems like how to leverage human and AI complementary strengths, how humans use assistive technologies and how we should design AI assistance, and the many misalignments between HAI systems in lab settings and real-world scenarios. Currently, I am interested in the following specific directions:



% For example, I learned that in human-AI teams, performance could be suboptimal due to psychological factors on the human side, like lack of cognitive engagement [\citenum{bucinca2021}] or the presence of assistance altering humans' decision-making process [\citenum{green2021}]. 
% Even if humans use AI assistance in “expected ways”, AI could still decrease human agency, as our work showed, or induce bias [\citenum{green2021}]. 
% I found the questions exciting, but moreover, I learned of the misalignments between HAI systems in lab settings and real-world scenarios, including misalignment in the task [\citenum{bucinca2020}], objective and outcomes [\citenum{{guerdan2022}}], in diseradata of the assistance [\citenum{yacoby2022}], and, a direction I am especially interested in, the experiment subjects. 
% I gradually developed a human-centered perspective where I would take a step back and question whether the task at hand is appropriate for real-world scenarios humans use in practice.



% I was used to pursuing an empirical objective like a benchmark, but disregarding the big picture elments of HAI like human and stakeholder needs, how (and should) human use AI, etc. These questions are immensly difficult and open-ended but are also what exictes me about this line of research.


%  For example, the explanations and assistance we ML researchers are designing may not be what real humans, including workers, stalkholders, need. 

% This alludes to a larger problem: the tradeoff between human agency and human-AI team performance. 
% Many human-AI teams provide humans with "persuasive" AI assistance that improves humans performance but at the expense of humans' overreliance and blind trust towards AI [\citenum{bansal2021}]. This is undesired and unethical, especially in high-stake domains where humans should have the last say.
% On the other hand, many work [\citenum{bansal2021}], including ours, also show that neutral, non-persuasive assistance that dissuade humans from blindly following AI perform no better than persuasive assistance. 
% Thus, I am interested in solving this dilemma.








% At UChicago, through Dr. Chenhao Tan's course in Human-Center Machine Learning I learned of the complexities of human-AI interaction. In high-stake domains like medical diagnosis, full automation of AI is often not desired and humans have to make the final decision. I am intrigued by the problems caused by this limitation, such as generating explanations that are informative and neutral instead of persuasive and deceiving, complementary performance etc. 



% In our project, we spent a large amount of time devising a decision support system that differed from existing literature providing model explanations: our goal was to provide faithful and neutral evidence for humans to make independent decisions. Thus we eventually devised a neutral decision support policy that did not reveal AI's predicted label and provided nearest-neighbor explanations from all classes. 
% We showed such a policy provided effective decision support, but it was less effective than a persuasive decision support policy closer to model explanations. 



% \subsect{direct impact}
% Also, I am motivated by the direct and visible impact in human-centered AI research. My research project [\citenum{human-compat}] has improved crowdworkers' performance on pneumonia diagnosis in chest X-rays, meaning it has the potential to be implemented in real-life medical settings. Such human-related experiments and improvements are very rewarding to me.
% % \\ 


% \subsect{interdisciplinary}
% Finally I enjoy the interdisciplinary aspects of human-centered AI research. 
% My past research in human-compatible AI decision-support involved modeling human perception, so we used triplet annotations, a method from experimental psychology. 
% My current research aims to leverage our human-compatible AI build radiology teaching framework and we are exploring psychology literature in learning and categorization. 
% More generally, human-centered AI revolves around how humans interact with a decision-making entity and thus involves many many different fields like economics, sociology, ethics, legal, etc. 
% An exciting aspect of human-centered AI resarch is learning and utilizing knowledge from multiple fields. 
% \\



