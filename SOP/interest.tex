%!TEX root = main.tex

\subsection*{Developing a human-centered perspective}



\sect{further development of interest}


\subsect{how it developed my interest}

Coming from a machine learning background, I initially treated our decision-support framework as another ML model, focusing on training better models and higher accuracies. But I soon learned that human-AI collaboration was more complicated than just chasing metrics as we encountered the tradeoff between human agency and performance: our highest-performing decision-support policy was a persuasive one that nudged humans to follow AI assistance and our neutral and informative explanation policy, while retaining more human agency, resulted in lower task performance.
I was intrigued by this dilemma and was drawn to the complexities of human-AI interaction. 
\subsect{sources: classes and shit}
I learned more about HAI through a \href{https://github.com/ChicagoHAI/human-centered-machine-learning}{Human-Centered Machine Learning course} and a \href{https://datascience.uchicago.edu/events/human-ai-conference/}{Human+AI conference}, both at UChicago, as well as from numerous related papers, gradually forming a new, more thorough perspective towards HAI.

\subsect{shift in perspective}
I first learned of the many ”non-ML” factors in HAI. 
For example, I learned that in human-AI teams, performance could be suboptimal due to psychological factors on the human side, like lack of cognitive engagement [\citenum{bucinca2021}] or the presence of assistance altering humans' decision-making process [\citenum{green2021}]. 
Even if humans use AI assistance in “expected ways”, AI could still decrease human agency, as our work showed, or induce bias [\citenum{green2021}]. 
I found the questions exciting, but moreover, I learned of the misalignments between HAI systems in lab settings and real-world scenarios, including misalignment in the task [\citenum{bucinca2020}], objective and outcomes [\citenum{{guerdan2022}}], in diseradata of the assistance [\citenum{yacoby2022}], and, a direction I am especially interested in, the experiment subjects. 
I gradually developed a human-centered perspective where I would take a step back and question whether the task at hand is appropriate for real-world scenarios humans use in practice.



% I was used to pursuing an empirical objective like a benchmark, but disregarding the big picture elments of HAI like human and stakeholder needs, how (and should) human use AI, etc. These questions are immensly difficult and open-ended but are also what exictes me about this line of research.


%  For example, the explanations and assistance we ML researchers are designing may not be what real humans, including workers, stalkholders, need. 

% This alludes to a larger problem: the tradeoff between human agency and human-AI team performance. 
% Many human-AI teams provide humans with "persuasive" AI assistance that improves humans performance but at the expense of humans' overreliance and blind trust towards AI [\citenum{bansal2021}]. This is undesired and unethical, especially in high-stake domains where humans should have the last say.
% On the other hand, many work [\citenum{bansal2021}], including ours, also show that neutral, non-persuasive assistance that dissuade humans from blindly following AI perform no better than persuasive assistance. 
% Thus, I am interested in solving this dilemma.








% At UChicago, through Dr. Chenhao Tan's course in Human-Center Machine Learning I learned of the complexities of human-AI interaction. In high-stake domains like medical diagnosis, full automation of AI is often not desired and humans have to make the final decision. I am intrigued by the problems caused by this limitation, such as generating explanations that are informative and neutral instead of persuasive and deceiving, complementary performance etc. 



% In our project, we spent a large amount of time devising a decision support system that differed from existing literature providing model explanations: our goal was to provide faithful and neutral evidence for humans to make independent decisions. Thus we eventually devised a neutral decision support policy that did not reveal AI's predicted label and provided nearest-neighbor explanations from all classes. 
% We showed such a policy provided effective decision support, but it was less effective than a persuasive decision support policy closer to model explanations. 



% \subsect{direct impact}
% Also, I am motivated by the direct and visible impact in human-centered AI research. My research project [\citenum{human-compat}] has improved crowdworkers' performance on pneumonia diagnosis in chest X-rays, meaning it has the potential to be implemented in real-life medical settings. Such human-related experiments and improvements are very rewarding to me.
% % \\ 


% \subsect{interdisciplinary}
% Finally I enjoy the interdisciplinary aspects of human-centered AI research. 
% My past research in human-compatible AI decision-support involved modeling human perception, so we used triplet annotations, a method from experimental psychology. 
% My current research aims to leverage our human-compatible AI build radiology teaching framework and we are exploring psychology literature in learning and categorization. 
% More generally, human-centered AI revolves around how humans interact with a decision-making entity and thus involves many many different fields like economics, sociology, ethics, legal, etc. 
% An exciting aspect of human-centered AI resarch is learning and utilizing knowledge from multiple fields. 
% \\



